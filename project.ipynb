{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Обрусистить корпус\n",
    "Задачи:\n",
    "1) script changes: диакритику ґ заменить на гх и др.\n",
    "2) lexical changes: какие-то мелкие слова типа як - как, двi - две и т.п.\n",
    "3) phonetic/morphological changes: ньска - нская, ньски - ньский, вi - ве, iв - ов, шт - ст, лн - льн, ця - ца, котр - котор, цьк - ск, ім - ом\n",
    "4) машинное обучение на определение i and ї"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Step 1. Get the list of words with i and ї from a sample corpus\n",
    "\n",
    "def WordsWithI():\n",
    "    \n",
    "    file = open('sample_corpus.txt', 'r', encoding='utf-8')\n",
    "    #words = re.findall('[^\\s]*i(?!werty)[^\\s]*', file) #[^\\s]*q(?!werty)[^\\s]*\n",
    "    words = re.findall('ˆ|\\s([a-zA-Zа-яА-ЯіїЇєЄґҐ]*i|I|і|ї[a-zа-яіїєґ]*)[.?,!\"()]|\\s|$', file) \n",
    "    \n",
    "    f = open('words with i', 'w', encoding='utf-8')\n",
    "    \n",
    "    for word in words:\n",
    "        f.write('%s\\t\\n'% (word))\n",
    "    f.close()\n",
    "    file.close()\n",
    "    \n",
    "WordsWithI()\n",
    "\n",
    "#Step 2. Handwork - put the equivalents in Russian and rename it to 'project_lexics_2'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "NOTES:\n",
    "#for word in file:\n",
    "#    word = word.strip().split(' ')\n",
    "    #word = re.sub('([?!,.—:;()])', '', word)\n",
    "    #word_list = re.findall('ˆ[[a-zA-Zа-яА-Я]*i[a-zA-Zа-яА-Я]*]*$', word)\n",
    "    #word_list = re.findall('/\\b[\\w]*?i[\\w]*?\\b/is', word)\n",
    "#    word_list = re.findall('^[a-zA-Z]*i(?!werty)*', word)\n",
    "#    print(word_list)\n",
    "    #for w in word_list:\n",
    "        #f.write('%s\\t\\n'% (w))\n",
    "#f.close()\n",
    "\n",
    "#w = re.sub(list(lexic.keys()), list(lexic.values()), w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Step 3. Make an aestetic work, using HW 2-4 (see 1-3 in goals). \n",
    "\n",
    "#Dictionaries Maker from hand-based analytics of the sample corpus.\n",
    "\n",
    "#Dictionary 1 with letters, morphemes, phonetic features to change:\n",
    "\n",
    "def Dictionaries():\n",
    "    letters_phon_morph = open('project_letters.txt', 'r', encoding='utf-8')\n",
    "    let_ph_mo = {}\n",
    "    for line in letters_phon_morph:\n",
    "        line = line.replace('\\n', '')\n",
    "        line = line.split('\\t')\n",
    "        let_ph_mo[line[0]] = line[1]\n",
    "    \n",
    "#Dictionary 2 with short lexems:    \n",
    "    lexics = open('project_lexics.txt', 'r', encoding='utf-8')\n",
    "    lexic = {}\n",
    "    for line in lexics:\n",
    "        line = line.replace('\\n', '')\n",
    "        line = line.split('\\t')\n",
    "        lexic[line[0]] = line[1]\n",
    "\n",
    "#Dictionary 3 based on Step 1:  \n",
    "    lexics_2 = open('project_lexics_2.txt', 'r', encoding='utf-8')\n",
    "    lexic2 = {}\n",
    "    for line in lexics_2:\n",
    "        line = line.replace('\\n', '')\n",
    "        line = line.split('\\t')\n",
    "        lexic2[line[0]] = line[1]\n",
    "        \n",
    "Dictionaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#'Cosmetic' code: changes phonetical, morhological and small lexical features\n",
    "\n",
    "def Step3():\n",
    "    \n",
    "    file = open('sample_corpus.txt', 'r', encoding='utf-8') #actually we can use wiki.txt here\n",
    "    f = open('cosmetic_wiki.txt', 'w', encoding='utf-8')\n",
    "\n",
    "    Dictionaries()\n",
    "    for word in file:\n",
    "        if word.strip() == '': continue\n",
    "        for key, value in lexic.items():\n",
    "            word = re.sub(key, value, word) #я хочу, чтобы менялось только полностью слово, а часть слова не была затронута\n",
    "        #for key, value in lexic2.items():\n",
    "            #project_lexics_2.txtword = re.sub(key, value, word)\n",
    "        word = word.strip().split(' ')\n",
    "        for w in word:\n",
    "            if w.strip() == '': continue\n",
    "            tr = w\n",
    "            for key, value in let_ph_mo.items():\n",
    "                tr = tr.replace(key, value)\n",
    "\n",
    "            f.write((tr) + ' ')\n",
    "        #print()\n",
    "    file.close()\n",
    "    f.close()\n",
    "    \n",
    "Step3()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Step 4. Train part. HW 4\n",
    "\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "#Преобразовать слово внутри списка в список, запомнить букву в списке по индексу и на этом же месте найти соответсвующую русскую букву\n",
    "#Ненужные буквы удалить, чтобы осталось только руссинские i и их русские эквиваленты\n",
    "#На выходе получится файл\n",
    "\n",
    "def Preparation():\n",
    "    \n",
    "    lines = open('project_lexics_2.txt', 'r', encoding='utf-8')\n",
    "    f = open('train.txt', 'w', encoding='utf-8')\n",
    "    letters = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if '\\t' not in line:\n",
    "            continue\n",
    "        line = re.sub('\\n', '', line)\n",
    "        row = line.split('\\t')\n",
    "        for r in row:\n",
    "            if r == '':\n",
    "                row.remove(r)\n",
    "            else:\n",
    "                break\n",
    "            for letter in r:\n",
    "                letters.append(letter)\n",
    "            \n",
    "            print(letters)\n",
    "            \n",
    "    lines.close()\n",
    "    f.close()\n",
    "\n",
    "Preparation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00\t3\ti\n",
      "\ti\n",
      "0.67\t4\tи\n",
      "\tі\n",
      "0.17\t1\tо\n",
      "\tі\n",
      "0.17\t1\tе\n",
      "\tі\n",
      "0.67\t2\tе\n",
      "\tї\n",
      "0.33\t1\tе\tї\n"
     ]
    }
   ],
   "source": [
    "#adobted spectei's codes\n",
    "\n",
    "def Train():\n",
    "    \n",
    "    # read all of the input lines in our trained meanings\n",
    "    lines = open('train.txt', 'r', encoding='utf-8')\n",
    "    f = open('project_model.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "    meaning_count = {}\n",
    "    letters = {}\n",
    "    letters_of_letter = {}\n",
    "    total = 0\n",
    "\n",
    "    for line in lines:\n",
    "        if '\\t' not in line:\n",
    "            continue\n",
    "        row = line.split('\\t')\n",
    "        \n",
    "        meaning = row[1]\n",
    "\n",
    "        # if we haven't seen the meaning before then initialise the count to 0\n",
    "        if meaning not in meaning_count:\n",
    "            meaning_count[meaning] = 0\n",
    "        meaning_count[meaning] = meaning_count[meaning] + 1\n",
    "\n",
    "        letter = row[0]\n",
    "        \n",
    "        if letter not in letters_of_letter: \n",
    "            letters_of_letter[letter] = 0\n",
    "        letters_of_letter[letter] = letters_of_letter[letter] + 1\n",
    "        \n",
    "        if letter not in letters:\n",
    "            letters[letter] = {}\n",
    "\n",
    "        if meaning not in letters[letter]:\n",
    "            letters[letter][meaning] = 0\n",
    "        letters[letter][meaning] = letters[letter][meaning] + 1\n",
    "        total = total + 1\n",
    "        #print(meaning_count[meaning], meaning, row)\n",
    "    # {'ADJ': 2, 'NOUN': 5, 'NUM': 2, '_': 80, 'PROPN': 3}                                                       \n",
    "\n",
    "    for meaning in meaning_count:\n",
    "        freq = meaning_count[meaning]/total\n",
    "        #print('%.2f\\t%d\\t%s\\t%s' % (freq, meaning_count[meaning], meaning, '_'))\n",
    "\n",
    "    # for each of the words in the matrix\n",
    "    for letter in letters:\n",
    "        # for each of the tag\n",
    "        for meaning in letters[letter]:\n",
    "            freq = letters[letter][meaning]/letters_of_letter[letter]\n",
    "            f.write('%.2f\\t%d\\t%s\\t%s' % (freq, letters[letter][meaning], meaning, letter))\n",
    "            print('%.2f\\t%d\\t%s\\t%s' % (freq, letters[letter][meaning], meaning, letter))\n",
    "\n",
    "\n",
    "    f.close()        \n",
    "    lines.close()\n",
    "Train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Step 5. Test. HW 5\n",
    "\n",
    "#spectei's codes\n",
    "\n",
    "def Test():\n",
    "\n",
    "    fd = open('project_model.tsv', 'r', encoding='utf-8')\n",
    "\n",
    "\n",
    "    frequent_tag = 'X' # variable to store the most frequent tag we have seen\n",
    "    last_max = 0 # variable to store the maximum frequency we have seen\n",
    "\n",
    "    word_tag = {}  # word_tag['cat'] = 'NOUN'\n",
    "    word_prob = {} # word_prob['cat'] = 0.6\n",
    "\n",
    "    for line in fd:\n",
    "        line = line.strip() # take away newline characters at the beginning/end of the line\n",
    "        line = line.split('\\t') # split the line on tab to make a list X \n",
    "        # ['0.17', '3', 'PROPN', '_']\n",
    "        if int(line[1]) > last_max: # if the frequency of the tag is higher than the previous maximum\n",
    "            last_max = int(line[1]) # set the new maximum\t\n",
    "            frequent_tag = line[2]\n",
    "\n",
    "        # if we have never seen the surface form before, the most frequent tag is the one we just saw\n",
    "        surface_form = line[3] # this is the surface form\n",
    "        prob = float(line[0]) # this is the probability\n",
    "        tag = line[2] # this is the tag\n",
    "        if surface_form not in word_tag: \n",
    "            word_tag[surface_form] = tag \n",
    "            word_prob[surface_form] = prob\n",
    "\n",
    "        # if we have seen the surface form before\n",
    "        if surface_form in word_tag:\n",
    "            # if the current probability is higher than the one we saw before\n",
    "            if prob > word_prob[surface_form]:\n",
    "                word_tag[surface_form] = tag\n",
    "                word_prob[surface_form] = prob\n",
    "\n",
    "    # read the input and assign the most frequent tag to each token\n",
    "\n",
    "    input = open('sample_corpus.txt', 'r', encoding='utf-8')\n",
    "    for line in input:\n",
    "        line = line.strip() # strip off excess whitespace \n",
    "        # if there is no tab in the line, print it out and skip it\t\n",
    "        if '\\t' not in line:\n",
    "            print(line)\n",
    "            continue\n",
    "        line = line.split('\\t')\n",
    "\n",
    "        # if we have seen the surface form before (line[1]) then use the most probable tag\n",
    "        if line[1] in word_tag:\n",
    "            line[3] = word_tag[line[1]]\n",
    "        # otherwise assign the most frequent tag to the input token\n",
    "        else:\n",
    "            line[3] = frequent_tag\n",
    "\n",
    "        # print out the tagged line\n",
    "        print('\\t'.join(line))\n",
    "        #print('%s\\t_\\t'% (line[1]) + '%s\\t_\\t_\\t_\\t_\\t_\\t' % (line[3]))\n",
    "\n",
    "    fd.close()\n",
    "\n",
    "Test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
